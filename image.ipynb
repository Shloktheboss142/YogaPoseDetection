{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File to use the trained model on yoga poses to predict the pose of a person from an image\n",
    "#### Author: Shlok Arjun Marathe\n",
    "#### Date: 6th December 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array([a.x, a.y])  \n",
    "    b = np.array([b.x, b.y])  \n",
    "    c = np.array([c.x, c.y])  \n",
    "\n",
    "    # Using the law of cosines to calculate the angle\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(np.degrees(radians))\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the correctness of the pose and provide feedback\n",
    "def provide_feedback(detected_pose, landmarks, ideal_angles):\n",
    "    feedback = []\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # Check the angle of each joint\n",
    "    for joint, ideal_angle in ideal_angles.items():\n",
    "        if joint == \"left_elbow\":\n",
    "            angle = calculate_angle(\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value],\n",
    "            )\n",
    "        elif joint == \"right_elbow\":\n",
    "            angle = calculate_angle(\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value],\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value],\n",
    "            )\n",
    "        elif joint == \"left_knee\":\n",
    "            angle = calculate_angle(\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_HIP.value],\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value],\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value],\n",
    "            )\n",
    "        elif joint == \"right_knee\":\n",
    "            angle = calculate_angle(\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value],\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value],\n",
    "            )\n",
    "\n",
    "        # Check if the angle is within the acceptable range\n",
    "        deviation = abs(angle - ideal_angle)\n",
    "        if deviation > 15:  \n",
    "            feedback.append(f\"{joint} angle is off by {deviation:.1f} degrees.\")\n",
    "    \n",
    "    # If no feedback is provided, the pose is correct\n",
    "    if not feedback:\n",
    "        feedback.append(\"Your pose looks great!\")\n",
    "    \n",
    "    return feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the correct angles for each pose\n",
    "ideal_pose_angles = {\n",
    "    \"Warrior\": {\n",
    "        \"left_elbow\": 180,\n",
    "        \"right_elbow\": 180,\n",
    "        \"left_knee\": 130,\n",
    "        \"right_knee\": 180,\n",
    "    },\n",
    "    \"Tree\": {\n",
    "        \"left_elbow\": 180,\n",
    "        \"right_elbow\": 180,\n",
    "        \"left_knee\": 180,\n",
    "        \"right_knee\": 45,\n",
    "    },\n",
    "    \"Plank\": {\n",
    "        \"left_elbow\": 90,\n",
    "        \"right_elbow\": 90,\n",
    "        \"left_knee\": 180,\n",
    "        \"right_knee\": 180,\n",
    "    },\n",
    "    \"Goddess\": {\n",
    "        \"left_elbow\": 90,\n",
    "        \"right_elbow\": 90,\n",
    "        \"left_knee\": 150,\n",
    "        \"right_knee\": 150,\n",
    "    },\n",
    "    \"Downward Dog\": {\n",
    "        \"left_elbow\": 180,\n",
    "        \"right_elbow\": 180,\n",
    "        \"left_knee\": 180,\n",
    "        \"right_knee\": 180,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shlok/Coding/RevoltronX/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/shlok/Coding/RevoltronX/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/jl/sw3zc9fx1m50mbphy9kfgpq00000gn/T/ipykernel_56552/249025156.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"yoga_pose_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model for yoga pose classification\n",
    "model = models.resnet18(pretrained=False)  \n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_features, 5)  \n",
    "model.load_state_dict(torch.load(\"yoga_pose_model.pth\"))  \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classes and the mediapipe objects\n",
    "class_names = [\"Downward Dog\", \"Goddess\", \"Plank\", \"Tree\", \"Warrior\"]\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing steps for the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and convert it to RGB\n",
    "image_path = \"./image1.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733675757.726831 28340308 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733675757.799860 28347258 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733675757.813057 28347263 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733675757.835452 28347260 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2024-12-09 00:35:57.969 Python[56552:28340308] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/jl/sw3zc9fx1m50mbphy9kfgpq00000gn/T/org.python.python.savedState\n",
      "2024-12-09 00:35:58.355 Python[56552:28340308] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-09 00:35:58.355 Python[56552:28340308] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "# Process the image with the pose model\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Draw landmarks on the image\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Extract landmarks\n",
    "        height, width, _ = image.shape\n",
    "        landmarks = [(int(lm.x * width), int(lm.y * height)) for lm in results.pose_landmarks.landmark]\n",
    "\n",
    "        # Classify pose\n",
    "        img_pil = Image.fromarray(image_rgb)\n",
    "        img_tensor = preprocess(img_pil).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            detected_pose = class_names[predicted.item()]\n",
    "\n",
    "        # Provide feedback\n",
    "        feedback = provide_feedback(detected_pose, results.pose_landmarks.landmark, ideal_pose_angles[detected_pose])\n",
    "\n",
    "        # Annotate the image with feedback\n",
    "        cv2.putText(image, f\"Pose: {detected_pose}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        y_offset = 60\n",
    "        for line in feedback:\n",
    "            cv2.putText(image, line, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            y_offset += 20\n",
    "\n",
    "        # Show the annotated image\n",
    "        cv2.imshow(\"Pose Detection with Feedback\", image)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
