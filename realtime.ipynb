{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File to use the trained model on yoga poses to predict the real-time pose of a person\n",
    "#### Author: Shlok Arjun Marathe\n",
    "#### Date: 6th December 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    \n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    # Use the dot product to calculate the angle\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    return np.degrees(angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the correctness of the pose and provide feedback\n",
    "def provide_feedback(detected_pose, landmarks, ideal_angles):\n",
    "    feedback = []\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # Check the angle of each joint\n",
    "    for joint, ideal_angle in ideal_angles.items():\n",
    "        if joint == \"left_elbow\":\n",
    "            angle = calculate_angle(\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value],\n",
    "            )\n",
    "        elif joint == \"right_elbow\":\n",
    "            angle = calculate_angle(\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value],\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value],\n",
    "            )\n",
    "        elif joint == \"left_knee\":\n",
    "            angle = calculate_angle(\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_HIP.value],\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value],\n",
    "                landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value],\n",
    "            )\n",
    "        elif joint == \"right_knee\":\n",
    "            angle = calculate_angle(\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value],\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value],\n",
    "            )\n",
    "\n",
    "        # Check if the angle is within the acceptable range\n",
    "        deviation = abs(angle - ideal_angle)\n",
    "        if deviation > 15:  \n",
    "            feedback.append(f\"{joint} angle is off by {deviation:.1f} degrees.\")\n",
    "    \n",
    "    # If no feedback is provided, the pose is correct\n",
    "    if not feedback:\n",
    "        feedback.append(\"Your pose looks great!\")\n",
    "    \n",
    "    return feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the correct angles for each pose\n",
    "ideal_pose_angles = {\n",
    "    \"Warrior\": {\n",
    "        \"left_elbow\": 180,\n",
    "        \"right_elbow\": 180,\n",
    "        \"left_knee\": 130,\n",
    "        \"right_knee\": 180,\n",
    "    },\n",
    "    \"Tree\": {\n",
    "        \"left_elbow\": 180,\n",
    "        \"right_elbow\": 180,\n",
    "        \"left_knee\": 180,\n",
    "        \"right_knee\": 45,\n",
    "    },\n",
    "    \"Plank\": {\n",
    "        \"left_elbow\": 90,\n",
    "        \"right_elbow\": 90,\n",
    "        \"left_knee\": 180,\n",
    "        \"right_knee\": 180,\n",
    "    },\n",
    "    \"Goddess\": {\n",
    "        \"left_elbow\": 90,\n",
    "        \"right_elbow\": 90,\n",
    "        \"left_knee\": 150,\n",
    "        \"right_knee\": 150,\n",
    "    },\n",
    "    \"Downward Dog\": {\n",
    "        \"left_elbow\": 180,\n",
    "        \"right_elbow\": 180,\n",
    "        \"left_knee\": 180,\n",
    "        \"right_knee\": 180,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shlok/Coding/RevoltronX/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/shlok/Coding/RevoltronX/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/jl/sw3zc9fx1m50mbphy9kfgpq00000gn/T/ipykernel_56590/249025156.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"yoga_pose_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model for yoga pose classification\n",
    "model = models.resnet18(pretrained=False)  \n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_features, 5)  \n",
    "model.load_state_dict(torch.load(\"yoga_pose_model.pth\"))  \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class names for the model\n",
    "class_names = [\"Downward Dog\", \"Goddess\", \"Plank\", \"Tree\", \"Warrior\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing steps for the input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733675641.596310 28340501 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733675641.672012 28340939 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733675641.686953 28340940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Load the Mediapipe pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 00:34:55.891 Python[56590:28340501] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "2024-12-09 00:34:57.441 Python[56590:28340501] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/jl/sw3zc9fx1m50mbphy9kfgpq00000gn/T/org.python.python.savedState\n",
      "W0000 00:00:1733675697.658456 28340939 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2024-12-09 00:34:57.910 Python[56590:28340501] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-09 00:34:57.910 Python[56590:28340501] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "# Start the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "# Process the webcam feed\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(img_rgb)\n",
    "\n",
    "    # Draw the landmarks and connections on the frame\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Extract landmarks\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        height, width, _ = frame.shape\n",
    "        landmarks = [(int(lm.x * width), int(lm.y * height)) for lm in landmarks]\n",
    "\n",
    "        # Classify pose\n",
    "        img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        img_tensor = preprocess(img_pil).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            detected_pose = class_names[predicted.item()]\n",
    "\n",
    "        # Provide feedback\n",
    "        feedback = provide_feedback(detected_pose, landmarks, ideal_pose_angles[detected_pose])\n",
    "\n",
    "        # Display pose name on the top-right corner\n",
    "        cv2.putText(frame, f\"Pose: {detected_pose}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display feedback on the frame\n",
    "        y_offset = 100\n",
    "        for line in feedback:\n",
    "            cv2.putText(frame, line, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            y_offset += 20\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Yoga Pose Detection with Feedback', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
